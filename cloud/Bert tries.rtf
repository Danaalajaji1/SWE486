{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # Import necessary Spark NLP components\
import sparknlp\
from sparknlp.base import DocumentAssembler\
from sparknlp.annotator import BertEmbeddings, Tokenizer\
from pyspark.ml.classification import LogisticRegression\
from pyspark.ml.evaluation import MulticlassClassificationEvaluator\
from pyspark.sql.functions import col\
from pyspark.ml.feature import StringIndexer\
from pyspark.ml import Pipeline\
\
# Start Spark NLP session\
spark = sparknlp.start()\
\
# Load your dataset\
parquet_file_path = "/Users/hoorrml/Desktop/dataset"\
df = spark.read.parquet(parquet_file_path)\
\
# Filter and select only necessary columns (e.g., 'plain_text' and 'categories')\
df_filtered = df.filter((col("language") == "ar") | (col("language") == "en"))\
df_selected = df_filtered.select("plain_text", "categories")\
\
# Take a sample (adjust for the 10 million record requirement)\
df_sample = df_selected.sample(fraction=0.10, seed=42)\
\
# Document Assembler, Tokenizer, and BERT embeddings\
document_assembler = DocumentAssembler() \\\
    .setInputCol("plain_text") \\\
    .setOutputCol("document")\
\
tokenizer = Tokenizer() \\\
    .setInputCols(["document"]) \\\
    .setOutputCol("token")\
\
bert_embeddings = BertEmbeddings.pretrained("bert_base_multilingual_cased", "xx") \\\
    .setInputCols(["document", "token"]) \\\
    .setOutputCol("embeddings")\
\
# Indexing the 'categories' column to create labels\
indexer = StringIndexer(inputCol="categories", outputCol="label")\
\
# Logistic Regression Classifier with one iteration\
classifier = LogisticRegression(featuresCol="embeddings", labelCol="label", maxIter=1)\
\
# Create a pipeline\
pipeline = Pipeline(stages=[\
    document_assembler,\
    tokenizer,\
    bert_embeddings,\
    indexer,\
    classifier\
])\
\
# Train-test split\
train_data, test_data = df_sample.randomSplit([0.8, 0.2], seed=123)\
\
# Train the model with one iteration\
model = pipeline.fit(train_data)\
\
# Make predictions\
predictions = model.transform(test_data)\
\
# Drop unnecessary columns after prediction (keeping only label and prediction for evaluation)\
predictions = predictions.select("label", "prediction")\
\
# Evaluate accuracy\
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")\
accuracy = evaluator.evaluate(predictions)\
\
# Evaluate precision\
evaluator.setMetricName("weightedPrecision")\
precision = evaluator.evaluate(predictions)\
\
# Print accuracy and precision\
print(f"Accuracy: \{accuracy\}")\
print(f"Precision: \{precision\}")\
\
One of Bert tries which also got to memory out of bound}